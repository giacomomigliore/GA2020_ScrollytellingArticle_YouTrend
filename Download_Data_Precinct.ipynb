{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get main page and link to all county pages\n",
    "url = \"https://results.enr.clarityelections.com/GA/105369/web.264614/#/access-to-races\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "# res = driver.find_element_by_xpath(\"/html/body/enr-root/div/main/div[2]/div/div[2]/enr-sidebar/div/enr-reports/div/div[2]/div[2]/div[2]/a\")\n",
    "# link = res.get_attribute(\"href\")\n",
    "URLlist = soup.find_all('a', attrs={'href': re.compile(\"^https://results.enr.clarityelections.com//GA/\")})\n",
    "URLs = []\n",
    "for link in URLlist:\n",
    "    if('.zip' not in str(link.get('href'))): URLs.append(str(link.get('href')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 done POLK\n",
      "115 done PULASKI\n",
      "116 done PUTNAM\n",
      "117 done QUITMAN\n",
      "118 done RABUN\n",
      "119 done RANDOLPH\n",
      "120 done RICHMOND\n",
      "121 done ROCKDALE\n",
      "122 done SCHLEY\n",
      "123 done SCREVEN\n",
      "124 done SEMINOLE\n",
      "125 done SPALDING\n",
      "126 done STEPHENS\n",
      "127 done STEWART\n",
      "128 done SUMTER\n",
      "129 done TALBOT\n",
      "130 done TALIAFERRO\n",
      "131 done TATTNALL\n",
      "132 done TAYLOR\n",
      "133 done TELFAIR\n",
      "134 done TERRELL\n",
      "135 done THOMAS\n",
      "136 done TIFT\n",
      "137 done TOOMBS\n",
      "138 done TOWNS\n",
      "139 done TREUTLEN\n",
      "140 done TROUP\n",
      "141 done TURNER\n",
      "142 done TWIGGS\n",
      "143 done UNION\n",
      "144 done UPSON\n",
      "145 done WALKER\n",
      "146 done WALTON\n",
      "147 done WARE\n",
      "148 done WARREN\n",
      "149 done WASHINGTON\n",
      "150 done WAYNE\n",
      "151 done WEBSTER\n",
      "152 done WHEELER\n",
      "153 done WHITE\n",
      "154 done WHITFIELD\n",
      "155 done WILCOX\n",
      "156 done WILKES\n",
      "157 done WILKINSON\n",
      "158 done WORTH\n"
     ]
    }
   ],
   "source": [
    "# Download .xls precinct-level detail results from each county\n",
    "i = 0\n",
    "for link in URLs:\n",
    "    driver.get(link)\n",
    "    time.sleep(10)\n",
    "    content = driver.page_source\n",
    "    \n",
    "    # Errors might arise. I usually repeat by putting in the for loop URLs[#:], where # is i. I also reassign i value\n",
    "    try:\n",
    "        res = driver.find_element_by_xpath(\"/html/body/enr-root/div/main/div[2]/div/div[2]/enr-sidebar/div/enr-reports/div/div[2]/div[2]/div[2]/a\")\n",
    "        downloadLink = res.get_attribute(\"href\")\n",
    "        span_county_name = driver.find_element_by_xpath(\"/html/body/enr-root/div/enr-header/nav/div/div[1]/enr-editor/span\")\n",
    "        county = span_county_name.text.split()[0]\n",
    "    except NoSuchElementException:\n",
    "      print(county + \" + 1 NoSuchElementException res = driver.find_element_by_xpath\")\n",
    "    except StaleElementReferenceException:\n",
    "      print(county + \" + 1 StaleElementReferenceException res = driver.find_element_by_xpath\")\n",
    "    except:\n",
    "      print(county + \" + 1 Something else went wrong res = driver.find_element_by_xpath\") \n",
    "    urllib.request.urlretrieve(downloadLink, './DownloadedXls/'+county+\".zip\")\n",
    "    time.sleep(2)\n",
    "    with ZipFile('./DownloadedXls/'+county+\".zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall('./DownloadedXls/')\n",
    "    old_file_name = './DownloadedXls/detail.xls'\n",
    "    new_file_name = './DownloadedXls/'+county+'.xls'\n",
    "    os.rename(old_file_name, new_file_name)\n",
    "    os.remove('./DownloadedXls/'+county+\".zip\")\n",
    "    i = i+1\n",
    "    print(str(i) + county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
